{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amara929/amara929/blob/main/ISTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yr3wQbEsnHDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Import Libraries"
      ],
      "metadata": {
        "id": "qD1-HyuinHw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2UjiHBZyEVg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given code imports TensorFlow and Keras, which are widely used deep learning frameworks, along with NumPy, a library for numerical computing in Python. tensorflow as tf allows access to TensorFlow’s functionalities, while from tensorflow import keras enables direct usage of Keras modules, which simplify building and training neural networks. import numpy as np brings in NumPy, which is essential for handling arrays and numerical operations, often used in preprocessing data for machine learning models. This setup is commonly used when developing deep learning models for tasks such as image recognition, natural language processing, and time series forecasting."
      ],
      "metadata": {
        "id": "u5LUfFxrqYjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rEBXzksEnQ3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Load the Buily"
      ],
      "metadata": {
        "id": "m_160Bc1nRms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the buily-in IMDB dataset\n",
        "imdb=keras.datasets.imdb"
      ],
      "metadata": {
        "id": "l3seB-hP2KR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imdb = keras.datasets.imdb imports the IMDB dataset module from Keras, which contains a preprocessed collection of movie reviews labeled as positive or negative for sentiment analysis. This dataset is often used for natural language processing (NLP) tasks, particularly binary classification problems. The dataset includes a predefined vocabulary where words are indexed based on their frequency in the dataset. By assigning it to the variable imdb, the code allows access to methods such as imdb.load_data(), which loads the dataset as tokenized sequences of integers, ready for use in training deep learning models like recurrent neural networks (RNNs) or long short-term memory (LSTM) networks."
      ],
      "metadata": {
        "id": "hl-TqDIfqiwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Set the vocabulary**"
      ],
      "metadata": {
        "id": "Pv70gmNKngL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocabulary size and maximum sequence length\n",
        "vocab_size=10000\n",
        "max_length=250"
      ],
      "metadata": {
        "id": "VHa15ydM83iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code vocab_size = 10000 and max_length = 250 sets two key parameters for processing text data in a deep learning model.\n",
        "\n",
        "vocab_size = 10000 limits the number of unique words in the dataset to the 10,000 most frequent words. This helps reduce computational complexity and memory usage by ignoring rare words that appear infrequently.\n",
        "\n",
        "max_length = 250 sets the maximum sequence length for each movie review to 250 words. If a review is longer, it will be truncated; if shorter, it may be padded to maintain a consistent input size for neural networks.\n",
        "\n",
        "\n",
        "These settings are essential for preparing text data before feeding it into models like LSTMs, GRUs, or Transformer-based architectures."
      ],
      "metadata": {
        "id": "G-ucwibFqtny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.0 Load the Dataset"
      ],
      "metadata": {
        "id": "JKWwvfT2noSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC2UA-pU9Jpo",
        "outputId": "e86e5706-a797-458b-bc8f-15d30aec7da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) loads the IMDB movie reviews dataset and splits it into training and testing sets.\n",
        "\n",
        "x_train and x_test contain the tokenized movie reviews, where each review is represented as a sequence of integers corresponding to words in a predefined vocabulary.\n",
        "\n",
        "y_train and y_test contain the sentiment labels for the reviews, where 0 represents a negative review and 1 represents a positive review.\n",
        "\n",
        "The argument num_words=vocab_size ensures that only the 10,000 most common words are included in the dataset, while rarer words are ignored.\n",
        "\n",
        "\n",
        "This step prepares the dataset for further preprocessing, such as padding or embedding, before training a deep learning model for sentiment analysis."
      ],
      "metadata": {
        "id": "dFs2y0_Xq_gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.0 Keras preprocessing**"
      ],
      "metadata": {
        "id": "pEYJuBbCnxAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to have the same length\n",
        "x_train=keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_length)\n",
        "x_test=keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_length)"
      ],
      "metadata": {
        "id": "HyVGNIau9anY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code pads or truncates the IMDB movie reviews to ensure that all sequences have the same length, making them suitable for deep learning models that require fixed-size inputs.\n",
        "\n",
        "keras.preprocessing.sequence.pad_sequences() is a Keras utility that adjusts the length of each sequence.\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length) processes the training data (x_train) by truncating longer reviews and padding shorter ones to a fixed length of max_length = 250.\n",
        "\n",
        "Similarly, x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length) applies the same transformation to the test data (x_test).\n",
        "\n",
        "\n",
        "Padding ensures that all sequences have the same size, which is essential for batch processing in deep learning models, especially for recurrent neural networks (RNNs), LSTMs, and GRUs."
      ],
      "metadata": {
        "id": "XVavrkmSrIm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Build the LSTM Model**"
      ],
      "metadata": {
        "id": "Kms_kK3on6_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "id": "I1auNmUT9_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given code defines a Long Short-Term Memory (LSTM) neural network using Keras' Sequential API for sentiment analysis on the IMDB dataset. Here's a breakdown of each layer:\n",
        "\n",
        "1. keras.Sequential([...]): This creates a sequential model where layers are stacked one after another.\n",
        "\n",
        "\n",
        "2. keras.layers.Embedding(vocab_size, 32):\n",
        "\n",
        "The Embedding layer converts word indices (integers) into dense vectors of fixed size (32 in this case).\n",
        "\n",
        "vocab_size = 10,000 defines the total number of words in the vocabulary.\n",
        "\n",
        "The output is a word embedding matrix where each word is represented as a 32-dimensional vector.\n",
        "\n",
        "\n",
        "\n",
        "3. keras.layers.LSTM(32):\n",
        "\n",
        "This adds an LSTM (Long Short-Term Memory) layer with 32 units to capture sequential dependencies in text data.\n",
        "\n",
        "LSTMs are well-suited for handling sequential data like text because they can retain important information over long sequences.\n",
        "\n",
        "\n",
        "\n",
        "4. keras.layers.Dense(1, activation='sigmoid'):\n",
        "\n",
        "A fully connected Dense layer with 1 neuron is added for binary classification (positive or negative sentiment).\n",
        "\n",
        "The sigmoid activation function outputs a probability score between 0 (negative sentiment) and 1 (positive sentiment).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This model learns to analyze the sentiment of movie reviews using word embeddings and LSTM-based sequence learning."
      ],
      "metadata": {
        "id": "71cB5zrWrTfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Compile the model**"
      ],
      "metadata": {
        "id": "quw6cEI8oKE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentropy',\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aaLP6CGa-tKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code compiles the LSTM model by specifying the optimizer, loss function, and evaluation metric before training.\n",
        "\n",
        "1. optimizer='adam':\n",
        "\n",
        "Uses the Adam (Adaptive Moment Estimation) optimizer, which is an efficient and widely used optimization algorithm for training deep learning models.\n",
        "\n",
        "It combines the benefits of momentum and adaptive learning rates for faster convergence.\n",
        "\n",
        "\n",
        "\n",
        "2. loss='binary_crossentropy':\n",
        "\n",
        "Since this is a binary classification problem (positive vs. negative sentiment), binary cross-entropy is used as the loss function.\n",
        "\n",
        "It calculates how well the model’s predicted probabilities match the actual labels (0 or 1).\n",
        "\n",
        "The loss function helps the model adjust its weights during training to minimize classification errors.\n",
        "\n",
        "\n",
        "\n",
        "3. metrics=['accuracy']:\n",
        "\n",
        "The accuracy metric is used to track model performance during training and evaluation.\n",
        "\n",
        "It measures the percentage of correctly predicted labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This compilation step prepares the model for training, ensuring it learns effectively from the IMDB dataset."
      ],
      "metadata": {
        "id": "Ertphut6rglZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Build the LSTM model**"
      ],
      "metadata": {
        "id": "ytfUTX1noTbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size,32),\n",
        "    keras.layers.LSTM(32, input_shape=(x_train.shape[1], 32)), # Specify input_shape for LSTM\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYCyOYcfB6Qe",
        "outputId": "f90c21db-b981-4329-ec55-730862fd0b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines an LSTM-based neural network using Keras' Sequential API for sentiment analysis on the IMDB dataset. The first layer, keras.layers.Embedding(vocab_size, 32), converts word indices into 32-dimensional dense vectors, enabling the model to learn word relationships. The keras.layers.LSTM(32, input_shape=(x_train.shape[1], 32)) layer processes the sequential text data, capturing long-term dependencies using 32 LSTM units; the input_shape=(x_train.shape[1], 32) explicitly defines the expected input dimensions, ensuring the model correctly interprets the padded sequences. Finally, keras.layers.Dense(1, activation='sigmoid') is a fully connected output layer with a single neuron, using the sigmoid activation function to produce a probability score between 0 (negative sentiment) and 1 (positive sentiment). This architecture enables the model to effectively classify movie reviews as either positive or negative based on learned patterns in the text data."
      ],
      "metadata": {
        "id": "JgKjSfmNsOoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.4 Compile the model**"
      ],
      "metadata": {
        "id": "UXhk4SJaoa-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentropy', # Corrected loss function name\n",
        "               metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FMNyeC9gB2-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "1. optimizer='adam':\n",
        "\n",
        "Uses the Adam optimizer, which is widely adopted in deep learning for its efficient handling of large datasets. Adam combines both momentum and adaptive learning rates, which helps in faster convergence and avoiding local minima.\n",
        "\n",
        "\n",
        "\n",
        "2. loss='binary_crossentropy':\n",
        "\n",
        "Since the model is performing binary classification (positive or negative sentiment), the loss function used is binary cross-entropy. This function measures the difference between the true labels (0 or 1) and the predicted probabilities, guiding the model to minimize classification errors during training.\n",
        "\n",
        "\n",
        "\n",
        "3. metrics=['accuracy']:\n",
        "\n",
        "The model tracks accuracy as its evaluation metric. Accuracy measures the proportion of correctly predicted labels (either positive or negative) over the total predictions, helping to monitor the model's performance during training and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This compilation step ensures the model is set up with the necessary configurations to begin learning from the data."
      ],
      "metadata": {
        "id": "xrcCuo4gsZ5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.5 Train the model**"
      ],
      "metadata": {
        "id": "Af3bmugGogiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,y_train,\n",
        "                  epochs=10,\n",
        "                  batch_size=32,\n",
        "                  validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eat_DSTnCXLu",
        "outputId": "8db26ce6-6904-4127-92e5-93e9dcbe46e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.7034 - loss: 0.5376 - val_accuracy: 0.8324 - val_loss: 0.3732\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8993 - loss: 0.2641 - val_accuracy: 0.8780 - val_loss: 0.3201\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9287 - loss: 0.1928 - val_accuracy: 0.8610 - val_loss: 0.3622\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9496 - loss: 0.1445 - val_accuracy: 0.8664 - val_loss: 0.3390\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9416 - loss: 0.1525 - val_accuracy: 0.8706 - val_loss: 0.3925\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9721 - loss: 0.0848 - val_accuracy: 0.8712 - val_loss: 0.4295\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9191 - loss: 0.1945 - val_accuracy: 0.7390 - val_loss: 0.5286\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9252 - loss: 0.1909 - val_accuracy: 0.8456 - val_loss: 0.4528\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.9819 - loss: 0.0587 - val_accuracy: 0.8642 - val_loss: 0.5124\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9800 - loss: 0.0583 - val_accuracy: 0.8498 - val_loss: 0.5451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. model.fit(): This method is used to train a machine learning model. The model learns from the data passed to it (x_train for input data and y_train for target labels).\n",
        "\n",
        "\n",
        "2. x_train: These are the training input features (usually images, text, etc.).\n",
        "\n",
        "\n",
        "3. y_train: These are the corresponding labels or target values for the training data.\n",
        "\n",
        "\n",
        "4. epochs=10: This means the model will iterate over the entire training dataset 10 times. Each complete pass through the training dataset is called an epoch.\n",
        "\n",
        "\n",
        "5. batch_size=32: This defines how many samples are processed before the model's internal parameters are updated. In this case, the model will process 32 samples at a time before making a change to its weights.\n",
        "\n",
        "\n",
        "6. validation_split=0.2: This specifies that 20% of the data should be set aside for validation, which is used to evaluate the model’s performance during training. The remaining 80% will be used for training.\n",
        "\n",
        "\n",
        "7. history: This variable stores the output of the training process, which includes training and validation metrics like loss and accuracy over each epoch. This can be used for plotting graphs and analyzing the model’s learning performance.\n",
        "\n",
        "\n",
        "\n",
        "In short, the code is training a model using the provided training data, running for 10 epochs, with a batch size of 32, and reserving 20% of the data for validation during training."
      ],
      "metadata": {
        "id": "Nl5cexB7uGTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.5 Evaluate the model**"
      ],
      "metadata": {
        "id": "DUD8IMvcookt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
        "print('Test accuracy:',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqo1ya-bBS55",
        "outputId": "5d336e35-4051-4d1a-c688-33b9d7cee9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8447 - loss: 0.5740\n",
            "Test accuracy: 0.8463199734687805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. model.evaluate(x_test, y_test):\n",
        "This function is used to evaluate the performance of the trained model on the test data (x_test, y_test).\n",
        "\n",
        "x_test: The features of the test dataset (input data).\n",
        "\n",
        "y_test: The true labels of the test dataset (expected output).\n",
        "\n",
        "\n",
        "The function returns two values:\n",
        "\n",
        "test_loss: The loss value (typically the error) between the predicted outputs and the true labels on the test data.\n",
        "\n",
        "test_acc: The accuracy of the model, which is a measure of how many predictions are correct out of the total predictions.\n",
        "\n",
        "\n",
        "\n",
        "2. print('Test accuracy:', test_acc):\n",
        "This line prints the accuracy (test_acc) of the model on the test dataset, giving an indication of how well the model generalizes to unseen data.\n",
        "\n",
        "\n",
        "\n",
        "In short, the code evaluates the model’s performance on the test set and prints the accuracy of the model."
      ],
      "metadata": {
        "id": "hCYQNSUNuUUb"
      }
    }
  ]
}